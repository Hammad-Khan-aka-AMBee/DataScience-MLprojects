{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of M.L.F1.3.ipynb","version":"0.3.2","provenance":[{"file_id":"1a8WyXqrX7s_JEm0yfCMr_VPj5JNSNnc3","timestamp":1564916518070}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"IWHklMdsGdwL","colab_type":"code","colab":{}},"source":["# add non-linear interaction term for a linear model\n","SMAxRSI = amd_df['14-day SMA'] * amd_df['14-day RSI']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"m-dvTbV1G9yz","colab_type":"code","colab":{}},"source":["amd_df['Adj_Volume_1d_change'] = amd_df['Adj_Volume'].pct_change()\n","one_day_change = amd_df['Adj_Volume_1d_change'].values\n","amd_df['Adj_Volume_1d_change_SMA'] = talib.SMA(one_day_change,\n","                                                timeperiod=10)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q6PxVtdFG-hq","colab_type":"code","colab":{}},"source":["print(amd_df.index.dayofweek)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uCRkeQqjHJ2l","colab_type":"code","colab":{}},"source":["days_of_week = pd.get_dummies(amd_df.index.dayofweek,\n","                                prefix='weekday',\n","                                drop_first=True)\n","print(days_of_week.head())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nf5OgAQhHQxU","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-dTpuzGiHpTv","colab_type":"text"},"source":["Feature engineering from volume\n","\n","We're going to use non-linear models to make more accurate predictions. With linear models, features must be linearly correlated to the target. Other machine learning models can combine features in non-linear ways. For example, what if the price goes up when the moving average of price is going up, and the moving average of volume is going down? The only way to capture those interactions is to either multiply the features, or to use a machine learning algorithm that can handle non-linearity (e.g. random forests).\n","\n","To incorporate more information that may interact with other features, we can add in weakly-correlated features. First we will add volume data, which we have in the lng_df as the Adj_Volume column."]},{"cell_type":"code","metadata":{"id":"RakdKHkjHrep","colab_type":"code","colab":{}},"source":["# Create 2 new volume features, 1-day % change and 5-day SMA of the % change\n","new_features = ['Adj_Volume_1d_change', 'Adj_Volume_1d_change_SMA']\n","feature_names.extend(new_features)\n","lng_df['Adj_Volume_1d_change'] = lng_df['Adj_Volume'].pct_change()\n","lng_df['Adj_Volume_1d_change_SMA'] = talib.SMA(lng_df['Adj_Volume_1d_change'].values,\n","                                               timeperiod=5)\n","\n","# Plot histogram of volume % change data\n","lng_df[new_features].plot(kind='hist', sharex=False, bins=50)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fh7CnuFoHzaC","colab_type":"code","colab":{}},"source":["#index27.svg"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9q8e9yK8H8iZ","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SuX-DfmXIHCH","colab_type":"text"},"source":["We can engineer datetime features to add even more information for our non-linear models. Most financial data has datetimes, which have lots of information in them -- year, month, day, and sometimes hour, minute, and second. But we can also get the day of the week, and things like the quarter of the year, or the elapsed time since some event (e.g. earnings reports).\n","\n","We are only going to get the day of the week here, since our dataset doesn't go back very far in time. The dayofweek property from the pandas datetime index will help us get the day of the week. Then we will dummy dayofweek with pandas' get_dummies(). This creates columns for each day of the week with binary values (0 or 1). We drop the first column because it can be inferred from the others."]},{"cell_type":"code","metadata":{"id":"qUuV4G9LIIa6","colab_type":"code","colab":{}},"source":["# Use pandas' get_dummies function to get dummies for day of the week\n","days_of_week = pd.get_dummies(lng_df.index.dayofweek,\n","                              prefix='weekday',\n","                              drop_first=True)\n","\n","# Set the index as the original DataFrame index for merging\n","days_of_week.index = lng_df.index\n","\n","# Join the dataframe with the days of week DataFrame\n","lng_df = pd.concat([lng_df, days_of_week], axis=1)\n","\n","# Add days of week to feature names\n","feature_names.extend(['weekday_' + str(i) for i in range(1, 5)])\n","lng_df.dropna(inplace=True)  # drop missing values in-place\n","print(lng_df.head())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9DmUgxZQIO34","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"19953ee7-9f02-4bb7-8792-b45bddf5b32e","executionInfo":{"status":"ok","timestamp":1564913840118,"user_tz":-300,"elapsed":781,"user":{"displayName":"Hammad Khan","photoUrl":"","userId":"06283299657089000525"}}},"source":["\"\"\"<script.py> output:\n","                Adj_Close  ...  weekday_4\n","    Date                   ...           \n","    2017-01-31      47.65  ...          0\n","    2017-02-01      47.10  ...          0\n","    2017-02-02      49.33  ...          0\n","    2017-02-03      49.43  ...          1\n","    2017-02-06      48.50  ...          0\n","    \n","    [5 rows x 19 columns]\n","\"\"\""],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<script.py> output:\\n                Adj_Close  ...  weekday_4\\n    Date                   ...           \\n    2017-01-31      47.65  ...          0\\n    2017-02-01      47.10  ...          0\\n    2017-02-02      49.33  ...          0\\n    2017-02-03      49.43  ...          1\\n    2017-02-06      48.50  ...          0\\n    \\n    [5 rows x 19 columns]\\n'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"6x0VPwXXIpvC","colab_type":"text"},"source":["Now that we have our volume and datetime features, we want to check the correlations between our new features (stored in the new_features list) and the target (5d_close_future_pct) to see how strongly they are related. Recall pandas has the built-in .corr() method for DataFrames, and seaborn has a nice heatmap() function to show the correlations."]},{"cell_type":"code","metadata":{"id":"F-_q4LA-Iqvm","colab_type":"code","colab":{}},"source":["# Add the weekday labels to the new_features list\n","new_features.extend(['weekday_' + str(i) for i in range(1, 5)])\n","\n","# Plot the correlations between the new features and the targets\n","sns.heatmap(lng_df[new_features + ['5d_close_future_pct']].corr(), annot=True)\n","plt.yticks(rotation=0)  # ensure y-axis ticklabels are horizontal\n","plt.xticks(rotation=90)  # ensure x-axis ticklabels are vertical\n","plt.tight_layout()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kEoo0wVxI0FD","colab_type":"code","colab":{}},"source":["#index28.svg"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vaNdnB6AI2qY","colab_type":"code","colab":{}},"source":["from sklearn.tree import DecisionTreeRegressor\n","\n","decision_tree = DecisionTreeRegressor(max_depth=5)\n","\n","decision_tree.fit(train_features, train_targets)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"of7_HGlMJ2v2","colab_type":"code","colab":{}},"source":["print(decision_tree.score(train_features, train_targets))\n","print(decision_tree.score(test_features, test_targets))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VCEzH7DPJ4f9","colab_type":"code","colab":{}},"source":["train_predictions = decision_tree.predict(train_features)\n","test_predictions = decision_tree.predict(test_features)\n","plt.scatter(train_predictions, train_targets, label='train')\n","plt.scatter(test_predictions, test_targets, label='test')\n","plt.legend()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ax5HEIXuLdFv","colab_type":"text"},"source":["Random forests are a go-to model for predictions; they work well out of the box. But we'll first learn the building block of random forests -- decision trees.\n","\n","Decision trees split the data into groups based on the features. Decision trees start with a root node, and split the data down until we reach leaf nodes.\n","\n","decision tree\n","\n","We can use sklearn to fit a decision tree with DecisionTreeRegressor and .fit(features, targets).\n","\n","Without limiting the tree's depth (or height), it will keep splitting the data until each leaf has 1 sample in it, which is the epitome of overfitting. We'll learn more about overfitting in the coming chapters.![alt text](![alt text](https://assets.datacamp.com/production/repositories/2168/datasets/0f5aaeefb8b871f1a358b54688114f735d8cfa9f/basic_decision_tree.jpg))"]},{"cell_type":"code","metadata":{"id":"4m6-KnIMLxRI","colab_type":"code","colab":{}},"source":["from sklearn.tree import DecisionTreeRegressor\n","\n","# Create a decision tree regression model with default arguments\n","decision_tree = DecisionTreeRegressor()\n","\n","# Fit the model to the training features and targets\n","decision_tree.fit(train_features, train_targets)\n","\n","# Check the score on train and test\n","print(decision_tree.score(train_features, train_targets))\n","print(decision_tree.score(test_features, test_targets))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9aiJbiYML2KK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"e43abc81-cefd-41f6-a4e5-8b076bf35962","executionInfo":{"status":"ok","timestamp":1564914725340,"user_tz":-300,"elapsed":952,"user":{"displayName":"Hammad Khan","photoUrl":"","userId":"06283299657089000525"}}},"source":["\"\"\"<script.py> output:\n","    0.9999958787531624\n","    -2.129303032663718\"\"\""],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<script.py> output:\\n    0.9999958787531624\\n    -2.129303032663718'"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"9SNXIwUhLfM8","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ddLwX6F5MAYr","colab_type":"text"},"source":["We always want to optimize our machine learning models to make the best predictions possible. We can do this by tuning hyperparameters, which are settings for our models. We will see in more detail how these are useful in future chapters, but for now think of them as knobs we can turn to tune our predictions to be as good as possible.\n","\n","For regular decision trees, probably the most important hyperparameter is max_depth. This limits the number of splits in a decision tree. Let's find the best value of max_depth based on the R2\n","score of our model on the test set, which we can obtain using the score() method of our decision tree models."]},{"cell_type":"code","metadata":{"id":"49AUeOOPMCGe","colab_type":"code","colab":{}},"source":["# Loop through a few different max depths and check the performance\n","for d in [3, 5, 10]:\n","    # Create the tree and fit it\n","    decision_tree = DecisionTreeRegressor(max_depth=d)\n","    decision_tree.fit(train_features, train_targets)\n","\n","    # Print out the scores on train and test\n","    print('max_depth=', str(d))\n","    print(decision_tree.score(train_features, train_targets))\n","    print(decision_tree.score(test_features, test_targets), '\\n')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VO7BmsDyMI22","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"8fdf59a7-fa75-44b2-9a93-1d3cef567541","executionInfo":{"status":"ok","timestamp":1564914792069,"user_tz":-300,"elapsed":1177,"user":{"displayName":"Hammad Khan","photoUrl":"","userId":"06283299657089000525"}}},"source":["\"\"\"<script.py> output:\n","    max_depth= 3\n","    0.313332035717818\n","    -0.3070692210378403 \n","    \n","    max_depth= 5\n","    0.5117261722974893\n","    -1.0602744418346384 \n","    \n","    max_depth= 10\n","    0.8989880793409756\n","    -1.3378328444389878\n","\"\"\""],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<script.py> output:\\n    max_depth= 3\\n    0.313332035717818\\n    -0.3070692210378403 \\n    \\n    max_depth= 5\\n    0.5117261722974893\\n    -1.0602744418346384 \\n    \\n    max_depth= 10\\n    0.8989880793409756\\n    -1.3378328444389878\\n'"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"V-FSUTC7MKoW","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qyviGiKTMQeY","colab_type":"text"},"source":["Once we have an optimized model, we want to check how it is performing in more detail. We already saw the R2\n","\n","score, but it can be helpful to see the predictions plotted vs actual values. We can use the .predict() method of our decision tree model to get predictions on the train and test sets.\n","\n","Ideally, we want to see diagonal lines from the lower left to the upper right. However, due to the simplicity of decisions trees, our model is not going to do well on the test set. But it will do well on the train set."]},{"cell_type":"code","metadata":{"id":"26Us8MY4MVPj","colab_type":"code","colab":{}},"source":["# Use the best max_depth of 3 from last exercise to fit a decision tree\n","decision_tree = DecisionTreeRegressor(max_depth=3)\n","decision_tree.fit(train_features, train_targets)\n","\n","# Predict values for train and test\n","train_predictions = decision_tree.predict(train_features)\n","test_predictions = decision_tree.predict(test_features)\n","\n","# Scatter the predictions vs actual values\n","plt.scatter(train_predictions, train_targets, label='train')\n","plt.scatter(test_predictions, test_targets, label='test')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vz8PYAMLMd5z","colab_type":"code","colab":{}},"source":["#index29.svg"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZD6tztugMhPW","colab_type":"code","colab":{}},"source":["from sklearn.ensemble import RandomForestRegressor\n","\n","random_forest = RandomForestRegressor()\n","random_forest.fit(train_features, train_targets)\n","print(random_forest.score(train_features, train_targets))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8anzfGX3NUWh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":256},"outputId":"fdd15980-a4ee-4eb0-eb99-ce1083ff5f86","executionInfo":{"status":"error","timestamp":1564915237872,"user_tz":-300,"elapsed":719,"user":{"displayName":"Hammad Khan","photoUrl":"","userId":"06283299657089000525"}}},"source":["random_forest = RandomForestRegressor(n_estimators=200,\n","                                      max_depth=5,\n","                                      max_features=4,\n","                                      random_state=42)\n","#learn about optimizing it well it is necessary b/f testing and training otherwise our model will overfit"],"execution_count":7,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-ab7c78a43833>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m random_forest = RandomForestRegressor(n_estimators=200,\n\u001b[0m\u001b[1;32m      2\u001b[0m                                       \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                       \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                       random_state=42)\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#learn about optimizing it well it is necessary b/f testing and training otherwise our model will overfit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'RandomForestRegressor' is not defined"]}]},{"cell_type":"code","metadata":{"id":"fLdVXn3lNgNy","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import ParameterGrid\n","\n","grid = {'n_estimators': [200], 'max_depth':[3, 5], 'max_features': [4, 8]}\n","\n","from pprint import pprint\n","\n","pprint(list(ParameterGrid(grid)))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LcmZZPHQN9xq","colab_type":"code","colab":{}},"source":["test_scores = []\n","\n","# loop through the parameter grid, set hyperparameters, save the scores\n","for g in ParameterGrid(grid):\n","    rfr.set_params(**g)  # ** is \"unpacking\" the dictionary\n","    rfr.fit(train_features, train_targets)\n","    test_scores.append(rfr.score(test_features, test_targets))\n","# find best hyperparameters from the test score and print\n","best_idx = np.argmax(test_scores)\n","print(test_scores[best_idx])\n","print(ParameterGrid(grid)[best_idx])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"43My0PgJONhI","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bze97xIwOXfy","colab_type":"text"},"source":["Data scientists often use random forest models. They perform well out of the box, and have lots of settings to optimize performance. Random forests can be used for classification or regression; we'll use it for regression to predict the future price change of LNG.\n","\n","We'll create and fit the random forest model similarly to the decision trees using the .fit(features, targets) method. With sklearn's RandomForestRegressor, there's a built-in .score() method we can use to evaluate performance. This takes arguments (features, targets), and returns the R2\n","score (the coefficient of determination)."]},{"cell_type":"code","metadata":{"id":"BgvgJrdkOZNo","colab_type":"code","colab":{}},"source":["from sklearn.ensemble import RandomForestRegressor\n","\n","# Create the random forest model and fit to the training data\n","rfr = RandomForestRegressor(n_estimators=200)\n","rfr.fit(train_features, train_targets)\n","\n","# Look at the R^2 scores on train and test\n","print(rfr.score(train_features, train_targets))\n","print(rfr.score(test_features, test_targets))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cEznEWa_Ohle","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"60dffb64-3a9b-4146-d714-fa53f74a30a4","executionInfo":{"status":"ok","timestamp":1564915421650,"user_tz":-300,"elapsed":819,"user":{"displayName":"Hammad Khan","photoUrl":"","userId":"06283299657089000525"}}},"source":["\"\"\"<script.py> output:\n","    0.8971734156648866\n","    -0.10300271777856396\"\"\""],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<script.py> output:\\n    0.8971734156648866\\n    -0.10300271777856396'"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"gYt0MmFzOkK5","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iHPsKLGPOqMs","colab_type":"text"},"source":["Tune random forest hyperparameters\n","\n","As with all models, we want to optimize performance by tuning hyperparameters. We have many hyperparameters for random forests, but the most important is often the number of features we sample at each split, or max_features in RandomForestRegressor from the sklearn library. For models like random forests that have randomness built-in, we also want to set the random_state. This is set for our results to be reproducible.\n","\n","Usually, we can use sklearn's GridSearchCV() method to search hyperparameters, but with a financial time series, we don't want to do cross-validation due to data mixing. We want to fit our models on the oldest data and evaluate on the newest data. So we'll use sklearn's ParameterGrid to create combinations of hyperparameters to search."]},{"cell_type":"code","metadata":{"id":"FiRBopuFOr96","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import ParameterGrid\n","\n","# Create a dictionary of hyperparameters to search\n","grid = {'n_estimators': [200], 'max_depth': [3], 'max_features': [4, 8], 'random_state': [42]}\n","test_scores = []\n","\n","# Loop through the parameter grid, set the hyperparameters, and save the scores\n","for g in ParameterGrid(grid):\n","    rfr.set_params(**g)  # ** is \"unpacking\" the dictionary\n","    rfr.fit(train_features, train_targets)\n","    test_scores.append(rfr.score(test_features, test_targets))\n","\n","# Find best hyperparameters from the test score and print\n","best_idx = np.argmax(test_scores)\n","print(test_scores[best_idx], ParameterGrid(grid)[best_idx])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6CpbLHyHOv6D","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"722d4947-d94d-4ea7-94dd-a6f4324761fc","executionInfo":{"status":"ok","timestamp":1564915494318,"user_tz":-300,"elapsed":867,"user":{"displayName":"Hammad Khan","photoUrl":"","userId":"06283299657089000525"}}},"source":["\"\"\"<script.py> output:\n","    0.048188439095540936 {'random_state': 42, 'n_estimators': 200, 'max_features': 4, 'max_depth': 3}\"\"\""],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"<script.py> output:\\n    0.048188439095540936 {'random_state': 42, 'n_estimators': 200, 'max_features': 4, 'max_depth': 3}\""]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"L9QMsxnOO11u","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HhYHQkTWO_vF","colab_type":"text"},"source":["Lastly, and as always, we want to evaluate performance of our best model to check how well or poorly we are doing. Ideally it's best to do back-testing, but that's an involved process we don't have room to cover in this course.\n","\n","We've already seen the R2\n","scores, but let's take a look at the scatter plot of predictions vs actual results using matplotlib. Perfect predictions would be a diagonal line from the lower left to the upper right."]},{"cell_type":"code","metadata":{"id":"PQkr6qFUPB2a","colab_type":"code","colab":{}},"source":["# Use the best hyperparameters from before to fit a random forest model\n","rfr = RandomForestRegressor(n_estimators=200, max_depth=3, max_features=4, random_state=42)\n","rfr.fit(train_features, train_targets)\n","\n","# Make predictions with our model\n","train_predictions = rfr.predict(train_features)\n","test_predictions = rfr.predict(test_features)\n","\n","# Create a scatter plot with train and test actual vs predictions\n","plt.scatter(train_targets, train_predictions, label='train')\n","plt.scatter(test_targets, test_predictions, label='test')\n","plt.legend()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yMYRP0A_PF1N","colab_type":"code","colab":{}},"source":["#index.30.svg\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zOk5IlA0PLt-","colab_type":"code","colab":{}},"source":["from sklearn.ensemble import RandomForestRegressor\n","\n","random_forest = RandomForestRegressor()\n","random_forest.fit(train_features, train_targets)\n","\n","feature_importances = random_forest.feature_importances_\n","\n","print(feature_importances"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WjpJvhh-Ra9X","colab_type":"code","colab":{}},"source":["# feature importances from random forest model\n","importances = random_forest.feature_importances_\n","\n","# index of greatest to least feature importances\n","sorted_index = np.argsort(importances)[::-1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"in5uvtUXRopm","colab_type":"code","colab":{}},"source":["x = range(len(importances))\n","# create tick labels\n","labels = np.array(feature_names)[sorted_index]\n","\n","plt.bar(x, importances[sorted_index], tick_label=labels)\n","\n","# rotate tick labels to vertical\n","plt.xticks(rotation=90)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sF9GRSB1Rqr6","colab_type":"code","colab":{}},"source":["from sklearn.ensemble import GradientBoostingRegressor\n","\n","gbr = GradientBoostingRegressor(max_features=4,\n","                                learning_rate=0.01,\n","                                n_estimators=200,\n","                                subsample=0.6,\n","                                random_state=42)\n","\n","gbr.fit(train_features, train_targets)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f0Z1VkDfSUgt","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tD3p6NQBSf6-","colab_type":"text"},"source":["One useful aspect of tree-based methods is the ability to extract feature importances. This is a quantitative way to measure how much each feature contributes to our predictions. It can help us focus on our best features, possibly enhancing or tuning them, and can also help us get rid of useless features that may be cluttering up our model.\n","\n","Tree models in sklearn have a .feature_importances_ property that's accessible after fitting the model. This stores the feature importance scores. We need to get the indices of the sorted feature importances using np.argsort() in order to make a nice-looking bar plot of feature importances (sorted from greatest to least importance)."]},{"cell_type":"code","metadata":{"id":"O5lQKbkpShya","colab_type":"code","colab":{}},"source":["# Get feature importances from our random forest model\n","importances = rfr.feature_importances_\n","\n","# Get the index of importances from greatest importance to least\n","sorted_index = np.argsort(importances)[::-1]\n","x = range(len(importances))\n","\n","# Create tick labels \n","labels = np.array(feature_names)[sorted_index]\n","plt.bar(x, importances[sorted_index], tick_label=labels)\n","\n","# Rotate tick labels to vertical\n","plt.xticks(rotation=90)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"amSHaL1PSlGu","colab_type":"code","colab":{}},"source":["#index31.svg"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FHFEK1QdSpvY","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}